---
title: "Holistic MLOps for better science"
subtitle: "PyData NYC 2022"
author: "Isabel Zimmerman, Posit PBC"
format:
  revealjs: 
    slide-number: true
    preview-links: auto
    theme: [simple, style.scss]
    width: 1600
    height: 900
---

# {background-image="images/rainbowroad.jpg"}


::: {.notes}
hello i am isabel!
i have a dog named toast who i teach to do lots of silly tricks!
i graduated from florida polytechnic with a degree in data science, and i decided to go back to school and am a graduate student at florida poly

to destress i do things like play video games... i wouldnt say im a GAMER, i mostly play things like mario kart with friends (if you're not familiar with mario kart, its a game where you get to drive go karts around on different tracks, one of the most famous is called rainbow road)
:::

# MLOps is...

. . .

a set of <u>practices</u> to *deploy* and *maintain* machine learning models in production **reliably** and **efficiently**

. . .

and these practices can be HARD.

::: {.notes}
- i said i started with Kubernetes-based + models. after a while where i STRUGGLED with the tools at hand, i felt that, as a data scientist, the tools were not built for me. they were built for a software engineer that had pretty deep knowledge of infrastructure--and i needed to productionalize models, but i didn't want to have to be a cloud architect as well a data scientist.

hands up, if you have deployed a model
keep it up if it was a DELIGHTFUL experience
:::

#

![](images/vetiverhex.png){fig-align="center"}

::: {.notes}
- so i changed career paths to build new tools designed specifically for data scientists to deploy models

- vetiver, which is an mlops framework to help data scientists in R and Python
:::

# 

![](images/ml_ops_cycle_no_ops.png)


::: {.notes}
put my student hat back on--
when you start learning about data science, you see an image that looks something like this: you learn about tools such as --

and so when you write data science code in Python using the packages and BEST practices you've learned, it goes something like this:
:::

#

```{python}
#| echo: true
#| output-location: slide

import pandas as pd
import numpy as np

np.random.RandomState(500)
raw = pd.read_csv('https://bit.ly/3sWty5A')
df = raw[["like_count", "funny", "show_product_quickly", \
    "celebrity", "danger", "animals"]].dropna()

df.head()
```

::: {.notes}
set seed for reproducibility
:::

#

```{.python code-line-numbers="12-16"}
import pandas as pd
import numpy as np

np.random.RandomState(500)
raw = pd.read_csv('https://bit.ly/3sWty5A')
df = raw[["like_count", "funny", "show_product_quickly", \
    "celebrity", "danger", "animals"]].dropna()

from sklearn import model_selection, preprocessing, ensemble

X_train, X_test, y_train, y_test = model_selection.train_test_split(
    df.drop(columns = ['like_count']),
    df['like_count'],
    test_size=0.2
)
```

::: {.notes}
split into training and test sets
:::

#

```{.python code-line-numbers="16-18"}
import pandas as pd
import numpy as np

np.random.RandomState(500)
raw = pd.read_csv('https://bit.ly/3sWty5A')
df = raw[["like_count", "funny", "show_product_quickly", \
    "celebrity", "danger", "animals"]].dropna()

from sklearn import model_selection, preprocessing, ensemble

X_train, X_test, y_train, y_test = model_selection.train_test_split(
    df.drop(columns = ['like_count']),
    df['like_count'],
    test_size=0.2
)
oe = preprocessing.OrdinalEncoder().fit(X_train)
rf = ensemble.RandomForestRegressor().fit(oe.transform(X_train), y_train)
rf_pipe = pipeline.Pipeline([('ordinal_encoder',oe), ('random_forest', rf)])
```

::: {.notes}
choose the right feature engineering for the job, and train your model

put these together in a pipeline
:::

# 

::: {.r-fit-text}
if you develop models...

:::

::: {.notes}
then i got to my first job and realized there's more to think about than splitting test/training/etc
you need to figure out how to share this model with teammates in a way that DOESNT include emailing your model to other people, you need to integrate it into some larger application (am i going to write this IN MY SHINY APP?), you need to ensure that it still performs well a week from now, or a month from now

you start to realize that these practices are not always enough!
:::

# 

::: {.r-fit-text}
*you probably should operationalize them*
:::

::: {.notes}
the business values of models generally 
:::


# 

![](images/ml_ops_cycle.png)

# {background-image="images/cc-select.jpg"}


::: {.notes}
enough data science, lets think about mario kart again.

jk, but really, its helpful to learn mlops this way.

- 50cc: slower
- 100cc: 
:::

# versioning

::: {.notes}
people think of versioning, its usually in the context of git! but we version lots of different things, and mostly badly
:::

## versioning {background-image="images/50.jpg"}

`model`

. . .

`model_final`

. . .

`model_final_final`

. . . 

`model_final_final_actually`

. . . 

`model_final_final_actually (1)`

::: {.notes}
versioning your model is the foundation for success in machine learning deployments...

we can already see here this is not going to scale for ONE MODEL
lacks context
:::

## versioning {background-image="images/50.jpg"} 

::: {.incremental}
- living in a central location
- discoverable by team
- load right into memory
:::

# {background-image="images/50.jpg"}

![](images/pinshex.png){fig-align="center"}

# {background-image="images/50.jpg"}

```{.python code-line-numbers="4-5"}
import pins
from vetiver import VetiverModel, vetiver_pin_write

model_board = pins.board_temp( # create place for models to be stored
    allow_pickle_read = True)

v = VetiverModel(rf_pipe, "ads")
```

# {background-image="images/50.jpg"}

```{.python code-line-numbers="4-5"}
import pins
from vetiver import VetiverModel, vetiver_pin_write

model_board = pins.board_temp( # can also be s3, azure, gcs, connect
    allow_pickle_read = True)

v = VetiverModel(rf_pipe, "ads")
```

# {background-image="images/50.jpg"}

```{.python code-line-numbers="7"}
import pins
from vetiver import VetiverModel, vetiver_pin_write

model_board = pins.board_temp( # create place for models to be stored
    allow_pickle_read = True)

v = VetiverModel(rf_pipe, "ads") # create deployable model object
```

# {background-image="images/50.jpg"}

```{.python code-line-numbers="8"}
import pins
from vetiver import VetiverModel, vetiver_pin_write

model_board = pins.board_temp( # create place for models to be stored
    allow_pickle_read = True)

v = VetiverModel(rf_pipe, "ads") # create deployable model object
vetiver_pin_write(model_board, v)
```

# {background-image="images/50.jpg"}

``` python
Meta(title='ads: a pinned Pipeline object',
    description="Scikit-learn <class 'sklearn.pipeline.Pipeline'> model", 
    created='20221102T094151Z', 
    pin_hash='4db397b49e7bff0b', 
    file='ads.joblib', 
    file_size=1087, 
    type='joblib', 
    api_version=1, 
    version=VersionRaw(version='65155'), 
    name='ads', 
    user={'required_pkgs': ['vetiver', 'scikit-learn']})
```

# {background-image="images/50.jpg"}

::: {.panel-tabset}

### Python
```{.python}
import pins
from vetiver import VetiverModel, vetiver_pin_write

model_board = pins.board_temp(
    allow_pickle_read = True)

v = VetiverModel(rf_pipe, "ads")
vetiver_pin_write(model_board, v)
```

### R

``` r
library(vetiver)
library(pins)

model_board <- board_temp()

v <- vetiver_model(rf, "ads")
model_board %>% 
  vetiver_pin_write(v)
```
:::

::: {.notes}
writing data to the pin
:::


## know what your input data should look like {background-image="images/100.jpg"}

- save a piece of your data to better debug when things go wrong

:::{.notes}
trying to figure out how to solve a puzzle is a lot harder when you don't know what the finished product looks like
:::

# {background-image="images/100.jpg"}

```{.python code-line-numbers="7"}
import pins
from vetiver import VetiverModel, vetiver_pin_write

model_board = pins.board_temp(
    allow_pickle_read = True)

v = VetiverModel(rf, "ads", ptype_data = X_train)
vetiver_pin_write(model_board, rf)
```

## utilizing model cards {background-image="images/150.jpg"}

not only good models, but _good_ models

- summary
- documentation
- fairness

::: {.notes}
from a team at google

kinda like writing down a recipe

this feels a bit different, since it is not as maybe TECHNICALLY involved
:::

## utilizing model cards {background-image="images/150.jpg"}

``` python
vetiver_pin_write(model_board, v)
```

## utilizing model cards {background-image="images/150.jpg"}

``` bash
Model Cards provide a framework for transparent, responsible reporting. 
 Use the vetiver `.qmd` Quarto template as a place to start, 
 with vetiver.model_card()
```

## utilizing model cards {background-image="images/150.jpg"}

``` python
vetiver.vetiver_pin_write(model_board, v)

vetiver.model_card()
```

## utilizing model cards {background-image="images/150.jpg"}

![](images/title.png)


## utilizing model cards {background-image="images/150.jpg"}

![](images/quant.png)

## utilizing model cards {background-image="images/150.jpg"}

![](images/ethics.png)

::: {.notes}
my model doesn't have any ethical challenges, it's predicting youtube likes

- Also, consider the possibility of gathering feedback from those impacted by the machine learning system, especially those with marginalized identities.

- However, we strongly advise that instead of deleting any such section because you have incomplete or imprecise information, you note your own process and considerations. 

The process of documenting the extent and limits of a machine learning system is part of transparent, responsible reporting. 

ultimately the extent of its value depends on you

actually, my dad has a quote that he always tells me--"if you haven't written it down, you haven't thought it out"
:::

#

![](images/mlops_version.jpg)

## deploy your model {background-image="images/50.jpg"}

![](images/deploy-cloud.jpg)

::: {.notes}
creating model as a REST API endpoint

useful bc model can be used in memory just like you loaded it! without having to load it

also useful  since API endpoints are testable, 
:::

## deploy your model {background-image="images/50.jpg"}

![](images/deploy-not-here.jpg)

## deploy your model {background-image="images/50.jpg"}

``` python
my_api = VetiverAPI(v)

my_api.run()
```

## deploy your model {background-image="images/50.jpg"}

``` python
vetiver.deploy_rsconnect(
    connect_server = connect_server, 
    board = model_board, 
    pin_name = "ads", 
    version = "59869")
```

## deploy your model {background-image="images/50.jpg"}

```python
vetiver.write_app(board=board, pin_name="ads", version = "59869")
vetiver.write_docker(app_file="app.py")
```
::: {.notes}
app.py file is going to read your pin, and use a vetiver model and then something called a vetiver api, which you can think of as a sort of model aware fastapi


with this docker container in hand, you can bring your model to numerous cloud services!

AWS ECR + ECS
Azure container registry and app service
:::


## model doesn't live in deployment {background-image="images/100.jpg"}

![](images/docker1.jpg)

::: {.notes}
your model is deployed, and you are happily using it!

have to think about some sophistication in the architecture.

might think you can put model in docker container, but i'm going to try to convince you this might not be the best choice

there are a few things that are useful to keep in mind 

:::

## model doesn't live in deployment {background-image="images/100.jpg"}

![](images/docker2.jpg)

::: {.notes}
when you start up your docker container, will load model into memory from pin in your s3 bucket, etc

:::

## model doesn't live in deployment {background-image="images/100.jpg"}

![](images/docker3.jpg)

::: {.notes}
your model is deployed, and you are happily using it! but, maybe you 

model lives inside container, this process is immediately painful
before you even update, container is larger. 
to update, you save model in container. if you are using versioned models inside your container, now you have two models in there (not going to be easily accessible to others to pin_read, unless you are ALSO storing them somewhere else). 
:::

## model doesn't live in deployment {background-image="images/100.jpg"}

![](images/docker4.jpg)

::: {.notes}
ad-hoc analysis of models over time. If the models exist inside containers, those containers have to be spun up and deployed to do analysis with them over multiple versions of models.

Monitoring usually involves more dependencies but doesn’t need to be as performant. 

Are you going to spin up another container from your “real” production image to use for monitoring purposes only? Probably don’t want to make HTTP calls to a real production container for your monitoring. 

For monitoring, it probably doesn’t really help to get new predictions over HTTP anyway (can predict in the same computational environment as where the validation data is).

model lives outside container. you pin your model to the board that lives on, say, s3. you change the version number inside your dockerfile, redeploy.

:::

#

![](images/mlops_deploy.jpg)

## monitoring {background-image="images/150.jpg"}

![](images/decay.jpeg)

::: {.notes}
model is deployed a data scientist's work is not done!

now, monitoring means somthing unique in MLOps-- not necessarily looking at CPU usage, runtime, etc, 

looking at statistical properties of input data or predictions
:::

## monitoring {background-image="images/150.jpg"}

```{.python code-line-numbers="1,10,17"}
metrics = vetiver.compute_metrics(
    new_data, 
    "date", 
    timedelta(weeks = 1), 
    [mean_absolute_error, r2_score], 
    "like_count", 
    "y_pred"
    )

vetiver.pin_metrics(
    model_board, 
    metrics, 
    "metrics_pin_name", 
    overwrite = True
    )

vetiver.plot_metrics(metrics)
```

::: {.notes}
we won't look super deep at these functions right, but 
:::

## monitoring {background-image="images/150.jpg"}

![](images/silent_error.jpeg)

::: {.notes}
it is SO IMPORTANT TO TRACK your model's performance metrics start decaying.

software engineering--when things went wrong, ERROR

models fail silently! and they can still run with no error, even if your accuracy is zero percent--

if you are not monitoring your model in some way, you are oblivious to decay.
:::

#

![](images/mlops_monitor.jpg)

# what's the difference between MLOps tools?


::: {.notes}
mlops is a happening space...if you google what the landscape looks like...
:::

#

![](https://46eybw2v1nh52oe80d3bi91u-wpengine.netdna-ssl.com/wp-content/uploads/2021/12/Data-and-AI-Landscape-2021-v3-small-1024x530.jpg){fig-align="center"}

## composability

. . . 

- VetiverModel
- VetiverAPI

::: {.notes}
because it's pretty 

few simple tools that you are able to compose together to make complex objects

since vetiverapi is built on fastapi, can build out to be quite complex
also has methods to add other POST endpoints

also is composable with other tools to build out a custom framework that works well for your team
:::

## ergonomics

- feels good to use
- works with the tools you like

::: {.notes}
one thing vetiver has worked really hard on is to lower the barrier to entry on deploying models, making this feel like a natural extension of your current data science workflow

you are still able to use the tools you want

also with leveraging pins, it makes it easy to move data between R and Python at places where this is possible
:::

## vetiver.rstudio.com or visit us at the Posit booth! 

![](images/summary.jpg)

::: {.notes}
in a composable and ergonomic way
:::

